# -*- coding: utf-8 -*-
"""Копия блокнота "Untitled0.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IGfhWXfm9xjTmHtAN2fXSFnJzk2YTvmj
"""

import re
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import *
from sklearn.model_selection import train_test_split
import pickle
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Список стоп-слов для русского языка
stopwords_rus = stopwords.words('russian')

# df_rus = pd.read_csv("dataset_with_syn.txt")
df_rus = pd.read_excel("Книга1.xlsx")
# df_rus = df_rus.dropna()

df_rus

import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.metrics import silhouette_score
import scipy.cluster.hierarchy as shc

df_rus[:10]

print(df_rus.shape)

# Предобработка данных
for i in range(df_rus.shape[0]):
  # Приводим текст к нижнему регистру
  string = str(df_rus.iloc[i][0])
  string = string.lower()
  print(string)

  # Уберём неинформативные данные (оставим только текст)
  string = re.sub("([^0-9A-Za-zА-ЯЁа-яё \t])|(\w+:\/\/\S+)", "", string)
  print(f"удаление неинформативных данных\n: {string}")

  # Токенизируем текст
  string = word_tokenize(string)
  print(f"Токенизация текста\n: {string}")

  # Удалим стоп-слова
  string_withoutstop = [word for word in string if word not in stopwords_rus]
  print(f"Удаление стоп-слов\n: {string_withoutstop}")

  # Лемматизируем (приведем к исходной форме) слова
  string = [WordNetLemmatizer().lemmatize(word) for word in string_withoutstop]
  print(f"Лемматизация текста\n: {string}")

  df_rus.iloc[i][0] = str(string)

# print(df_rus.iloc[:, 0])

print(df_rus.iloc[:, 0])

X = df_rus.iloc[:, 0]
# Handling the missing values
X.fillna(method ='ffill', inplace = True)

# Векторизуем
vectorizer = CountVectorizer(ngram_range=(1, 3))
# векторизуем обучающую выборку
X_ = vectorizer.fit_transform(X)

X_

# Scaling the data so that all the features become comparable
scaler = StandardScaler(with_mean=False)
X_scaled = scaler.fit_transform(X_)

# Normalizing the data so that the data approximately
# follows a Gaussian distribution
X_normalized = normalize(X_scaled)

# Converting the numpy array into a pandas DataFrame
# X_normalized = pd.DataFrame(X_normalized)

X_normalized

#Text Clustering:
#let's start with PCA(principle component anaysis)
from sklearn.decomposition import PCA
variances = []
#now finding the number of components which alteast satisfy half variance
for i in range (1,100,5):
    pca = PCA(i)
    pca.fit(X_.toarray())
    variances.append(pca.explained_variance_ratio_.sum())

#plotting the variances to find the number of components:
import matplotlib.pyplot as plt
plt.figure(figsize = (8, 8))
plt.grid()
x= range(1,100,5)
plt.plot(x, variances, 'b*')
plt.xlabel('number of components')
plt.ylabel('Variance')
plt.title('finding the number of components')
plt.show()

#the number of components obtained are 60
#now performing pca for 60 components
pca = PCA(70)
pca.fit(X_.toarray())
docs_pca = pca.transform(X_.toarray())

!pip install scikit-learn

docs_pca_labels = ['crude', 'sugar', 'coffee', 'ship', 'gold']

from sklearn.cluster import KMeans
k_means = KMeans(5, max_iter =100)
clusters = k_means.fit_predict(X_)

#visualising the k means
plt.figure(figsize = (8,8))
plt.grid()
sns.scatterplot(docs_pca[:, 0], docs_pca[:, 1], hue = clusters)
plt.show()

#let's plot the PCA clusters
import seaborn as sns
plt.figure(figsize = (8, 8 ))
plt.grid()
sns.scatterplot(docs_pca[:, 0], docs_pca[:, 1], hue =data_labels_encode)
plt.show()



pca = PCA(n_components = 2)
X_principal = pca.fit_transform(X_normalized)
X_principal = pd.DataFrame(X_principal)
X_principal.columns = ['P1', 'P2']

ac2 = AgglomerativeClustering(n_clusters = 2)

# Visualizing the clustering
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
           c = ac2.fit_predict(X_principal), cmap ='rainbow')
plt.show()

"""# Обработка данных"""

import re
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import *
from sklearn.model_selection import train_test_split
import pickle
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Список стоп-слов для русского языка
stopwords_rus = stopwords.words('russian')

df = pd.read_excel("hack_data_without names and last column.xlsx")
df

# объединение всех столбцов, кроме последнего в один
columns_to_combine = df.iloc[:, :3]  # Здесь берутся все столбцы, если нужно выбрать конкретные, можно указать их индексы

# Объединение данных всех столбцов в один
combined_column = pd.concat([columns_to_combine[col] for col in columns_to_combine.columns], ignore_index=True)

# Создание нового DataFrame с объединённым столбцом
df = pd.DataFrame({'combined': combined_column})
df = df.dropna()
df

!python -m spacy download ru_core_news_sm

import spacy

# Функция для извлечения ключевых фраз (1-3 слова)
def extract_key_phrases(text):
    doc = nlp(text)
    key_phrases = []
    # Ищем фразы от 1 до 3 слов
    for n in range(1, 4):  # n - количество слов в фразе
        for i in range(len(doc) - n + 1):
            phrase = ' '.join([token.lemma_ for token in doc[i:i+n] if not token.is_stop and not token.is_punct])
            if len(phrase.split()) > 0:
                key_phrases.append(phrase)
    return key_phrases

# Функция для удаления именованных сущностей
def remove_named_entities(text):
    doc = nlp(text)
    filtered_tokens = []
    for token in doc:
        # Удаляем только именованные сущности (PER, ORG, GPE и т.д.)
        if token.ent_type_ not in ["PER", "ORG", "GPE", "LOC"]:
            filtered_tokens.append(token.text)
        else:
            filtered_tokens.append(" ")

    return " ".join(filtered_tokens)

# Предобработка данных датафрейма
df_new = []
df_key_phrases = []
# Загрузка русскоязычной модели spaCy
nlp = spacy.load("ru_core_news_sm")
for i in range(df.shape[0]):
    # Приводим текст к нижнему регистру
    string = str(df.iloc[i][0])
    string = string.lower()
    # print("string", string)

    # Уберём неинформативные данные (оставим только текст)
    string = re.sub("([^0-9А-ЯЁа-яё \t])|(\w+:\/\/\S+)", " ", string)
    # print(f"удаление неинформативных данных\n: {string}")
    if string != '':
      # удаление имен собственных



      string = remove_named_entities(string)

      # Извлечение ключевых фраз из очищенных текстов
      df_key_phrases.append(extract_key_phrases(string))

      # Токенизируем текст
      string = word_tokenize(string)
      # print(f"Токенизация текста\n: {string}")

      # Удалим стоп-слова
      string_withoutstop = [word for word in string if word not in stopwords_rus]
      # print(f"Удаление стоп-слов\n: {string_withoutstop}")

      # Лемматизируем (приведем к исходной форме) слова
      string = [WordNetLemmatizer().lemmatize(word) for word in string_withoutstop]
      # print(f"Лемматизация текста\n: {string}")

      df_new.append(string)

    else:
      pass
df_new = pd.Series(df_new)
df_key_phrases = pd.Series(df_key_phrases)

df_new

df_key_phrases

for i in range(df_key_phrases.shape[0]):
  print(type(df_key_phrases.iloc[i]))
  df_key_phrases.iloc[i] = str(df_key_phrases.iloc[i])

df_key_phrases

from sklearn.feature_extraction.text import TfidfVectorizer

# Векторизация данных (ключевых фраз !!!!!)
# Векторизация текстов с помощью TF-IDF на основе ключевых фраз
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df_key_phrases)

"""scaler
normilizing
"""

# Разделение данных на обучающую и тестовую выборки
# test_size = 0.2
# seed = 7
# X_train, X_test = train_test_split(tfidf_matrix, test_size=test_size, random_state=seed)

# print(X_train.shape)



"""Кластеризация"""

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN

df_clusterization = pd.DataFrame(columns=['df_key_phrases', 'kmeans_cluster'])
print(df_clusterization)
# Кластеризация с помощью K-means
kmeans = KMeans(n_clusters=15, random_state=42)

df_clusterization['df_key_phrases'] = df_key_phrases
df_clusterization['kmeans_cluster'] = kmeans.fit_predict(tfidf_matrix)

clusters = kmeans.labels_.tolist()

print(df_clusterization)
print(clusters)

list_of_key_phrases_cluster_1 = []

for i in range(df_clusterization.shape[0]):
  if df_clusterization['kmeans_cluster'][i] == 1:
    print(type(df_clusterization.iloc[i]['df_key_phrases']))
    list_of_key_phrases_cluster_1.append(df_clusterization.iloc[i]['df_key_phrases'])


# Извлечение единой ключевой фразы для кластера
# Функция для извлечения ключевых фраз (1-3 слова)
def extract_key_phrases(lst):
  sum_str = ''
  for sublist in lst:
    print(list(sublist))
    # for word in sublist:
    #   # print(word)
    #   sum_str += word
    # for word in sublist:
    #   print(word)
    #   sum_str += word
  # print(sum_str)
extract_key_phrases(list_of_key_phrases_cluster_1)
#     doc = nlp(text)
#     key_phrases = []
#     # Ищем фразы от 1 до 3 слов
#     for n in range(1, 4):  # n - количество слов в фразе
#         for i in range(len(doc) - n + 1):
#             phrase = ' '.join([token.lemma_ for token in doc[i:i+n] if not token.is_stop and not token.is_punct])
#             if len(phrase.split()) > 0:
#                 key_phrases.append(phrase)
#     return key_phrases
# print(list_of_key_phrases_cluster_1)

# Сохранение модели
import pickle

model = KMeans(n_clusters=15, random_state=0, n_init="auto").fit(tfidf_matrix)
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

"""PCA (метод главных компонент) — это метод снижения размерности данных. Его основная цель — уменьшить количество переменных (фичей), сохранив при этом как можно больше информации. PCA преобразует исходные переменные в набор новых, называемых главными компонентами, которые являются линейными комбинациями исходных переменных. Эти компоненты ранжируются по вкладу в дисперсию данных, поэтому можно выбрать лишь первые несколько, которые объясняют основную часть дисперсии."""

#k-means
clusterkm = kmeans.labels_.tolist()


frame = pd.DataFrame(df_clusterization['df_key_phrases'], index = [clusterkm])

#k-means
out = { 'title':df_clusterization['df_key_phrases'], 'cluster': clusterkm }
frame1 = pd.DataFrame(out, index = [clusterkm], columns = ['title', 'cluster'])

cluster_names = df_clusterization('kmeans_cluster')['title'].first().to_dict()
df_clusterization['cluster_name'] = df_clusterization['kmeans_cluster'].map(cluster_names)

print(frame1['cluster'].value_counts().value_counts())
#Сначала необходимо вычислить расстояние между векторами. Для этого будет применяться косинусовое расстояние. В статьях предлагают использовать вычитание из единицы, чтобы не было отрицательных значений и находилось в пределах от 0 до 1, поэтому сделаем так же:
from sklearn.metrics.pairwise import cosine_similarity
dist = 1 - cosine_similarity(tfidf_matrix)
dist.shape

# Метод главных компонент - PCA
from sklearn.decomposition import IncrementalPCA
icpa = IncrementalPCA(n_components=2, batch_size=16)
get_ipython().magic('time icpa.fit(dist) #demo =')
get_ipython().magic('time demo2 = icpa.transform(dist)')
xs, ys = demo2[:, 0], demo2[:, 1]

# PCA 3D
from sklearn.decomposition import IncrementalPCA
icpa = IncrementalPCA(n_components=3, batch_size=16)
get_ipython().magic('time icpa.fit(dist) #demo =')
get_ipython().magic('time ddd = icpa.transform(dist)')
xs, ys, zs = ddd[:, 0], ddd[:, 1], ddd[:, 2]

#Можно сразу примерно посмотреть, что получится в итоге
#from mpl_toolkits.mplot3d import Axes3D
#fig = plt.figure()
#ax = fig.add_subplot(111, projection='3d')
#ax.scatter(xs, ys, zs)
#ax.set_xlabel('X')
#ax.set_ylabel('Y')
#ax.set_zlabel('Z')
#plt.show()



#можно сгенерировать цвета для кластеров
import random
def generate_colors(n):
    color_list = []
    for c in range(0,n):
        r = lambda: random.randint(0,255)
        color_list.append( '#%02X%02X%02X' % (r(),r(),r()) )
    return color_list

#устанавливаем цвета
#cluster_colors = colors = sns.color_palette("tab20", 15)  # Используем палитру с 20 различными цветами
# {0: '#ff0000', 1: '#ff0066', 2: '#ff0099',  3: '#ff00cc', 4: '#ff00ff',}
#даем имена кластерам, но из-за рандома пусть будут просто 01234
#cluster_names = {0: '0',  1: '1', 2: '2',  3: '3', 4: '4',5: '5',6: '6',7: '7',8: '8',9: '9',10: '10',11: '11',12: '12',13: '13',14: '14',15: '15'}
#matplotlib inline

#создаем data frame, который содержит координаты (из PCA) + номера кластеров и сами запросы
df = pd.DataFrame(dict(x=xs, y=ys, label=clusterkm, title=df_clusterization['df_key_phrases']))
#группируем по кластерам
groups = df.groupby('label')

fig, ax = plt.subplots(figsize=(72, 36)) #figsize подбирается под ваш вкус

# Определение числовых имен для 15 кластеров
cluster_names = [str(i) for i in range(15)]  # Имена кластеров: от 0 до 14

# Настройка цветов для 15 кластеров
colors = sns.color_palette("tab20", 15)  # Используем палитру с 15 различными цветами

for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=colors[name], mec='none')
    ax.set_aspect('auto')
    ax.tick_params(        axis= 'x',
        which='both',
        bottom='off',
        top='off',
        labelbottom='off')
    ax.tick_params(        axis= 'y',
        which='both',
        left='off',
        top='off',
        labelleft='off')


ax.legend(numpoints=1)  #показать легенду только 1 точки
#добавляем метки/названия в х,у позиции с поисковым запросом
#for i in range(len(df_clusterization)):
#    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=6)

#показать график
plt.show()
plt.close()

# Кластеризация с помощью Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=5)
df['agglo_cluster'] = agglo.fit_predict(tfidf_matrix.toarray())
get_ipython().magic('time answer = agglo.fit_predict(tfidf_matrix.toarray())')
answer.shape

# Кластеризация с помощью DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=2)
df['dbscan_cluster'] = dbscan.fit_predict(tfidf_matrix.toarray())
labels = dbscan.labels_
labels.shape

#dbscan
clusters3 = labels
 #agglo
clusters4 = answer.tolist()


#dbscan
out = { 'title': df_clusterization['df_key_phrases'], 'cluster': clusters3 }
frame2 = pd.DataFrame(out, index = [clusters3], columns = ['title', 'cluster'])

 #agglo
out = { 'title': df_clusterization['df_key_phrases'], 'cluster': clusters4 }
frame3 = pd.DataFrame(out, index = [clusters4], columns = ['title', 'cluster'])


print(frame2['cluster'].value_counts(),frame3['cluster'].value_counts())

# Метод главных компонент - PCA

from matplotlib import rc


#создаем data frame, который содержит координаты (из PCA) + номера кластеров и сами запросы
df2 = pd.DataFrame(dict(x=xs, y=ys, label=clusters3, title=df_clusterization['df_key_phrases']))
#группируем по кластерам
groups = df.groupby('label')

fig, ax = plt.subplots(figsize=(72, 36)) #figsize подбирается под ваш вкус

# Определение числовых имен для 15 кластеров
cluster_names = [str(i) for i in range(15)]  # Имена кластеров: от 0 до 14

# Настройка цветов для 15 кластеров
colors = sns.color_palette("tab20", 15)  # Используем палитру с 15 различными цветами

for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=colors[name], mec='none')
    ax.set_aspect('auto')
    ax.tick_params(        axis= 'x',
        which='both',
        bottom='off',
        top='off',
        labelbottom='off')
    ax.tick_params(        axis= 'y',
        which='both',
        left='off',
        top='off',
        labelleft='off')


ax.legend(numpoints=1)  #показать легенду только 1 точки
#добавляем метки/названия в х,у позиции с поисковым запросом
for i in range(len(df/16)):
    ax.text(df2.iloc[i]['x'], df2.iloc[i]['y'], df2.iloc[i]['title'], size=6)

#показать график
plt.show()
plt.close()



"""Визуализация кластеров"""

import seaborn as sns
#visualising the k means
cluster_names = [str(i) for i in range(15)]  # Имена кластеров: от 0 до 14

# Настройка цветов для 15 кластеров
colors = sns.color_palette("tab20", 15)  # Используем палитру с 15 различными цветами

# Визуализация кластеров для K-means
plt.figure(figsize=(12, 6))
sns.countplot(data=df_clusterization, x='kmeans_cluster', palette=colors)
plt.title('Распределение ответов по кластерам K-means (15 кластеров)')
plt.xlabel('Кластер')
plt.ylabel('Количество текстов')


# Присвоение имен каждому кластеру в DataFrame
df_clusterization['cluster_name'] = df_clusterization['kmeans_cluster'].apply(lambda x: df_clusterization.iloc[x]['title'])
plt.figure(figsize = (8,8))
plt.grid()
sns.scatterplot(docs_pca[:, 0], docs_pca[:, 1], hue = clusters)

# Указываем числовые имена кластеров как подписи
plt.xticks(ticks=range(15), labels=cluster_names)
plt.show()

plt.show()

!sudo ipython -m pip install mpld3

import mpld3
# Plot
fig, ax = plt.subplots(figsize=(25,27))
ax.margins(0.03)


for name, group in groups:
    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, #ms=18
                     label=cluster_names[name], mec='none',
                     color=colors[name])
    ax.set_aspect('auto')
    labels = [i for i in group.title]

    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels, voffset=10, hoffset=10)
    mpld3.plugins.connect(fig, tooltip) #   , TopToolbar()

    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])

    #ax.axes.get_xaxis().set_visible(False)
    #ax.axes.get_yaxis().set_visible(False)

ax.set_title("K-Means", size=20) #groups

ax.legend(numpoints=1)
mpld3.disable_notebook()
#mpld3.display()
mpld3.save_html(fig, "km.html")

mpld3.show()
#mpld3.save_json(fig, "vivod.json")
#mpld3.fig_to_html(fig)



labels = ['point {0}'.format(i + 1) for i in range(N)]
tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=labels)
mpld3.plugins.connect(fig, tooltip)

mpld3.show()
fig, ax = plt.subplots(figsize=(72,36))
for name, group in groups:
    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18,
                     label=cluster_names[name], mec='none',
                     color=cluster_colors[name])
    ax.set_aspect('auto')
    labels = [i for i in group.title]
    tooltip = mpld3.plugins.PointLabelTooltip(points, labels=labels)
    mpld3.plugins.connect(fig, tooltip)

ax.set_title("K-means", size=20)

mpld3.display()
